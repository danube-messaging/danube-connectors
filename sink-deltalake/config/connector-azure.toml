# Delta Lake Sink Connector - Azure Blob Storage Configuration Example
#
# This example shows how to stream events from Danube to Delta Lake tables
# stored in Azure Blob Storage.

connector_name = "deltalake-sink-azure"
danube_service_url = "http://localhost:6650"

[deltalake]
# Storage backend
storage_backend = "azure"

# Azure Configuration
azure_storage_account = "mystorageaccount"
azure_container = "delta-tables"

# Azure credentials from environment:
# - AZURE_STORAGE_ACCOUNT_KEY or AZURE_STORAGE_SAS_TOKEN

# Global batch settings (can be overridden per topic)
batch_size = 100           # Flush after 100 messages
flush_interval_ms = 3000    # OR flush after 3 seconds (whichever comes first)

#######################
# IoT Sensor Data Topic
#######################
[[deltalake.topic_mappings]]
topic = "/iot/sensors"
subscription = "deltalake-sensors"
delta_table_path = "abfss://delta-tables@mystorageaccount.dfs.core.windows.net/sensors"
write_mode = "append"
include_danube_metadata = true

# Schema validation (schema already exists on topic via producer/admin)
expected_schema_subject = "iot-sensor-data-v1"

# Field mappings: JSON path â†’ Delta Lake column
field_mappings = [
    { json_path = "sensor_id", column = "sensor_id", data_type = "Utf8", nullable = false },
    { json_path = "temperature", column = "temperature", data_type = "Float64", nullable = false },
    { json_path = "humidity", column = "humidity", data_type = "Float64", nullable = false },
    { json_path = "pressure", column = "pressure", data_type = "Float64", nullable = true },
    { json_path = "timestamp", column = "timestamp", data_type = "Timestamp", nullable = false },
]
