# Delta Lake Sink Connector - Google Cloud Storage Configuration Example
#
# This example shows how to stream events from Danube to Delta Lake tables
# stored in Google Cloud Storage.

connector_name = "deltalake-sink-gcs"
danube_service_url = "http://localhost:6650"

[deltalake]
# Storage backend
storage_backend = "gcs"

# GCP Configuration
gcp_project_id = "my-gcp-project"

# GCP credentials from environment:
# - GOOGLE_APPLICATION_CREDENTIALS (path to service account JSON file)

# Global batch settings (can be overridden per topic)
batch_size = 100           # Flush after 100 messages
flush_interval_ms = 3000    # OR flush after 3 seconds (whichever comes first)

#######################
# Analytics Events Topic
#######################
[[deltalake.topic_mappings]]
topic = "/analytics/events"
subscription = "deltalake-analytics"
delta_table_path = "gs://my-bucket/tables/analytics"
write_mode = "append"
include_danube_metadata = true

# Schema validation (schema already exists on topic via producer/admin)
expected_schema_subject = "analytics-events-v1"

# Field mappings: JSON path â†’ Delta Lake column
field_mappings = [
    { json_path = "event_id", column = "event_id", data_type = "Utf8", nullable = false },
    { json_path = "user_id", column = "user_id", data_type = "Utf8", nullable = false },
    { json_path = "event_type", column = "event_type", data_type = "Utf8", nullable = false },
    { json_path = "event_data", column = "event_data", data_type = "Utf8", nullable = true },
    { json_path = "timestamp", column = "timestamp", data_type = "Timestamp", nullable = false },
]
