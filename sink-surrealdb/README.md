# SurrealDB Sink Connector

Stream events from Danube into [SurrealDB](https://surrealdb.com/) - the ultimate multi-model database for modern applications. Built entirely in Rust for maximum performance and zero JVM overhead.

## âœ¨ Features

- ğŸ”’ **Schema Validation** - Validate messages against registered JSON schemas
- ğŸš€ **Multi-Model Support** - Store events as documents or time-series data
- â±ï¸ **Time-Series Optimization** - Automatic timestamp handling for temporal queries
- ğŸ¯ **Multi-Topic Routing** - Route different topics to different tables with independent configurations
- ğŸ“¦ **Configurable Batching** - Optimize throughput with per-topic batch sizes and flush intervals
- ğŸ”„ **Subscription Types** - Shared, Exclusive, or FailOver subscription modes
- ğŸ”‘ **Custom Record IDs** - Use message attributes for idempotent inserts or auto-generate
- ğŸ“ **Metadata Enrichment** - Optionally include Danube metadata (topic, offset, timestamp)
- âš¡ **Zero-Copy Performance** - Rust-to-Rust with WebSocket protocol
- ğŸ›¡ï¸ **Production Ready** - Health checks, metrics, graceful shutdown

**Supported Storage Modes:**
- **Document** - Regular document storage (default)
- **TimeSeries** - Time-series data with automatic timestamp handling

**Use Cases:** Real-time analytics, event sourcing, time-series data, document storage, operational databases

## ğŸš€ Quick Start

### Running with Docker

```bash
docker run -d \
  --name surrealdb-sink \
  -v $(pwd)/connector.toml:/etc/connector.toml:ro \
  -e CONNECTOR_CONFIG_PATH=/etc/connector.toml \
  -e DANUBE_SERVICE_URL=http://danube-broker:6650 \
  -e CONNECTOR_NAME=surrealdb-sink \
  -e SURREALDB_URL=ws://surrealdb:8000 \
  -e SURREALDB_USERNAME=root \
  -e SURREALDB_PASSWORD=root \
  danube/sink-surrealdb:latest
```

**Note:** All structural configuration (topics, tables, schema validation, batching) must be in `connector.toml`. See [Configuration](#configuration) section below.

### Complete Example

For a complete working setup with Docker Compose, test data, and query examples:

ğŸ‘‰ **See [sink-surrealdb example](example/README.md)**

The example includes:
- Docker Compose setup (Danube + ETCD + SurrealDB + Schema Registry)
- Schema registration and validation
- Pre-configured connector.toml with v0.2.0 features
- Test producers using danube-cli
- Query examples and data verification

## âš™ï¸ Configuration

### ğŸ“– Complete Configuration Guide

See **[config/README.md](config/README.md)** for comprehensive configuration documentation including:
- Core and connector-specific configuration options
- Schema validation setup (v0.2.0)
- Subscription types and storage modes
- Environment variable reference
- Configuration examples

### ğŸ“„ Quick Reference

#### Environment Variables

Environment variables are used **only for secrets and connection URLs**:

| Variable | Description | Use Case |
|----------|-------------|----------|
| `CONNECTOR_CONFIG_PATH` | Path to TOML config file | **Required** |
| `DANUBE_SERVICE_URL` | Danube broker URL | Override for different environments |
| `CONNECTOR_NAME` | Unique connector name | Override for different deployments |
| `SURREALDB_URL` | SurrealDB connection URL | Override for different environments (dev/staging/prod) |
| `SURREALDB_USERNAME` | Database username | **Secrets** - should not be in config files |
| `SURREALDB_PASSWORD` | Database password | **Secrets** - should not be in config files |

**All other configuration (topics, tables, schema validation, batching) must be in the TOML file.**

#### TOML Configuration (Required)

All connector configuration must be defined in a TOML file:

```toml
connector_name = "surrealdb-sink"
danube_service_url = "http://localhost:6650"
metrics_port = 9090

[surrealdb]
url = "ws://localhost:8000"
namespace = "production"
database = "events"
username = "admin"
password = "password"

batch_size = 100
flush_interval_ms = 1000

# Route user events to user_events table with schema validation
[[surrealdb.topic_mappings]]
topic = "/events/user"
subscription = "surrealdb-user"
subscription_type = "Shared"
table_name = "user_events"
expected_schema_subject = "user-events-v1"  # Schema validation
storage_mode = "Document"
include_danube_metadata = true
batch_size = 200

# Route IoT data to sensor_readings table (time-series)
[[surrealdb.topic_mappings]]
topic = "/iot/sensors"
subscription = "surrealdb-iot"
subscription_type = "Shared"
table_name = "sensor_readings"
expected_schema_subject = "sensor-v1"  # Schema validation
storage_mode = "TimeSeries"  # Adds _timestamp field
include_danube_metadata = true
batch_size = 500
flush_interval_ms = 2000
```

See [config/README.md](config/README.md) for complete examples and detailed documentation.

## ğŸ› ï¸ Development

### Building

```bash
# Build release binary
cargo build --release

# Run tests
cargo test

# Build Docker image
docker build -t danube/sink-surrealdb:latest .
```

### Schema Validation

Validate messages against registered JSON schemas:

```toml
[[surrealdb.topic_mappings]]
topic = "/events/user"
subscription = "surrealdb-user"
subscription_type = "Shared"
table_name = "user_events"
expected_schema_subject = "user-events-v1"  # Enable validation
storage_mode = "Document"
```

**Benefits:**
- âœ… **Data Quality** - Invalid messages rejected at source
- âœ… **Type Safety** - Guaranteed message structure
- âœ… **Early Detection** - Validation errors caught at producer
- âœ… **Automatic Deserialization** - Runtime handles JSON parsing

**How it works:**
1. Register schema in Danube Schema Registry
2. Create topic with `--schema-subject user-events-v1`
3. Producer validates messages before sending
4. Connector receives pre-validated, deserialized JSON

**Example Schema (`user-events-v1`):**
```json
{
  "type": "object",
  "properties": {
    "user_id": {"type": "string"},
    "event_type": {"type": "string"},
    "timestamp": {"type": "string", "format": "date-time"}
  },
  "required": ["user_id", "event_type"]
}
```

**Stored in SurrealDB:**
```json
{
  "user_id": "user-123",
  "event_type": "login",
  "timestamp": "2024-01-01T12:00:00Z"
}
```

### Storage Modes

The connector supports two storage modes for different use cases:

### Document Mode (Default)

Regular document storage for general-purpose data:

```toml
storage_mode = "Document"
```

**Use for:**
- User events
- Application logs
- General JSON documents
- Unstructured data

### TimeSeries Mode

Optimized for time-series data with automatic timestamp handling:

```toml
storage_mode = "TimeSeries"
```

**Features:**
- Adds `_timestamp` field to every record
- Uses Danube `publish_time` (set when message was published)
- Optimized for temporal queries
- No payload parsing required

**Example Configuration:**

```toml
[[surrealdb.topic_mappings]]
topic = "/iot/temperature"
subscription = "surrealdb-iot"
subscription_type = "Shared"
table_name = "temperature_readings"
expected_schema_subject = "sensor-v1"
storage_mode = "TimeSeries"  # Uses Danube publish_time
batch_size = 500
```

**Input Payload:**
```json
{
  "sensor_id": "sensor-001",
  "temperature": 23.5
}
```

**Stored in SurrealDB:**
```json
{
  "sensor_id": "sensor-001",
  "temperature": 23.5,
  "_timestamp": "2024-01-01T12:00:00.123Z"
}
```

**Timestamp Source:**
- Uses Danube `publish_time` (microseconds since epoch)
- Set automatically when message is published to Danube
- Converted to RFC3339 format for storage

**Use for:**
- IoT sensor data
- Metrics and monitoring
- System logs with timestamps
- Financial tick data

### Metadata Enrichment

Optionally include Danube metadata for audit trails:

```toml
include_danube_metadata = true
```

**Adds metadata to each record:**

```json
{
  "user_id": "user-123",
  "event_type": "login",
  "_danube_metadata": {
    "danube_topic": "/events/user",
    "danube_offset": 12345,
    "danube_timestamp": "2024-01-01T12:00:01Z",
    "danube_message_id": "topic:/events/user/producer:1/offset:12345"
  }
}
```

**Benefits:**
- Message traceability
- Event ordering
- Debugging
- Data lineage


### Record ID Management

### Auto-Generated IDs (Default)

SurrealDB generates unique IDs automatically:

```rust
// Producer doesn't set record_id attribute
producer.send(payload, None).await?;
```

**Result:** `events:âŸ¨uuidâŸ©`

### Custom Record IDs (Idempotent)

Set `record_id` attribute in the producer:

```rust
let mut attributes = HashMap::new();
attributes.insert("record_id".to_string(), "order-12345".to_string());
producer.send(payload, Some(attributes)).await?;
```

**Result:** `orders:order-12345`

**Benefits:**
- Idempotent inserts (reprocessing updates existing record)
- Predictable record IDs
- Easy lookups
- Natural keys from source systems

### Performance Tuning

#### Batch Size

Control throughput vs latency:

```toml
# High throughput (batch more records)
batch_size = 500
flush_interval_ms = 5000

# Low latency (flush frequently)
batch_size = 10
flush_interval_ms = 100
```

**Recommendations:**
- **High-volume streams**: `batch_size = 500-1000`
- **Real-time analytics**: `batch_size = 50-100`
- **Transactional data**: `batch_size = 10-20`

#### Connection Protocol

Use WebSocket for better performance:

```toml
url = "ws://surrealdb:8000"  # Recommended
# url = "http://surrealdb:8000"  # HTTP fallback
```

#### Per-Topic Optimization

Different topics can have different performance profiles:

```toml
# Logs: high volume, larger batches
[[surrealdb.topic_mappings]]
topic = "/logs"
batch_size = 1000
flush_interval_ms = 5000

# Orders: low volume, fast flush
[[surrealdb.topic_mappings]]
topic = "/orders"
batch_size = 10
flush_interval_ms = 100
```

### Monitoring

#### Prometheus Metrics

The connector exposes metrics on port `9090` (configurable):

```bash
curl http://localhost:9090/metrics
```

**Key Metrics:**
- `danube_connector_records_processed_total` - Total records processed
- `danube_connector_batches_flushed_total` - Total batches flushed
- `danube_connector_errors_total` - Total errors encountered
- `danube_connector_batch_size` - Current batch size histogram
- `danube_connector_flush_duration_seconds` - Flush duration histogram

#### Health Checks

Check connector health:

```bash
# Via metrics endpoint
curl http://localhost:9090/health

# Via logs
docker logs -f surrealdb-sink
```

## ğŸ“š Documentation

### Complete Working Example

See **[example/README.md](example/README.md)** for a complete setup with:
- Docker Compose (Danube + ETCD + SurrealDB + Schema Registry)
- Schema registration and validation
- Test producers using danube-cli v0.6.1+
- Single and multi-topic configurations
- Query examples and monitoring

### Configuration Examples

- **[config/connector.toml](config/connector.toml)** - Fully documented reference configuration
- **[config/README.md](config/README.md)** - Complete configuration guide

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Danube Broker   â”‚
â”‚  Topic: /events â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Stream messages
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SurrealDB Sink  â”‚
â”‚   Connector     â”‚
â”‚  - Batch        â”‚
â”‚  - Transform    â”‚
â”‚  - Route        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Insert records
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SurrealDB     â”‚
â”‚  Multi-Model DB â”‚
â”‚  - Documents    â”‚
â”‚  - Graphs       â”‚
â”‚  - Time-Series  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”§ Troubleshooting

### Schema Validation Errors

**Error:** `Schema validation failed`

**Solution:**
- Check schema is registered: `danube-admin-cli schemas list`
- Verify message matches schema requirements
- Ensure topic has correct `schema-subject` configured

### Connection Issues

**Error:** `Failed to connect to SurrealDB`

**Solution:**
- Verify SurrealDB is running: `curl http://localhost:8000/health`
- Check connection URL format: `ws://host:port` or `http://host:port`
- Verify credentials if authentication is enabled

### No Data in SurrealDB

**Solution:**
- Check connector logs: `docker logs surrealdb-sink`
- Verify messages are being sent to Danube
- Check batch settings - data may be buffered
- Confirm table name and namespace/database settings

## ğŸ“š References

- [SurrealDB Documentation](https://surrealdb.com/docs)
- [Danube Messaging](https://github.com/danube-messaging/danube)
- [Configuration Guide](config/README.md)
- [Example Setup](example/README.md)
